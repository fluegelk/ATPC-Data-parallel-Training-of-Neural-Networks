\documentclass[conference,compsoc,a4paper]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}


% configs from ba
\usepackage[english]{babel}
\usepackage[raiselinks=true,
			bookmarks=true,
			bookmarksopenlevel=1,
			bookmarksopen=true,
			bookmarksnumbered=true,
			hyperindex=true,
			plainpages=false,
			pdfpagelabels=true,
			pdfborder={0 0 0.5},
			colorlinks=false,
			linkbordercolor={0 0.61 0.50},
			citebordercolor={0 0.61 0.50}]{hyperref}
\addto\extrasenglish{%
  \def\chapterautorefname{Chapter}%
  \def\chapterrefname{Chapter}%
  \def\sectionautorefname{Section}%
  \def\sectionrefname{Section}%
  \def\subsectionautorefname{Section}%
  \def\subsectionrefname{Section}%
  \def\subsubsectionautorefname{Section}%
  \def\subsubsectionrefname{Section}%
  \def\paragraphautorefname{Paragraph}%
  \def\paragraphrefname{Paragraph}%
}

\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newcounter{MYtempeqncnt}

\begin{document}

\title{Data-parallel Training of Neural Networks}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Katharina Flügel}
\IEEEauthorblockA{TODO?}}


% make the title area
\maketitle

% General rule: no math, special symbols or citations in the abstract
\begin{abstract}
With an increase in both model and data size, training neural networks is increasingly more time-consuming.
Techniques to parallelize and distribute the training in order to reduce these training times are thus ever more important.
This seminar paper surveys different strategies for both distribution and parallelization when training neural networks.
Two essential principles to distinguish different parallelization approaches are model and data parallelism.
While we describe the general concept of model parallelism, this paper focuses mostly on data parallelism.
We illustrate multiple data-parallel algorithms, including both synchronous and asynchronous communication schemes.

Furthermore, we implement synchronous SGD, a data-parallel training technique based on synchronous communication via MPI-AllReduce.
We test this implementation by training LeNet5 on the image classification datasets MNIST and CIFAR10.
Using four GPUs, we achieve speedups of up to X. % TODO set speedup
With 16 CPUs on a shared-memory machine, we even reach speedups of Y. % TODO set speedup
We can observe several hardware influences, especially on the GPU, including the general speedup of GPUs compared to the CPU, the communication overhead of GPU-CPU-communication, and possible effects of PCI-E piggybacking.
Additionally, we evaluate the effects of the batch size on both sequential training and scalability.
Our results conform to the notion that while larger batch sizes reduce the training time per epoch and increase the scalability, they reduce the convergence per epoch at the same time.

\end{abstract}

\section{Introduction} % (fold)
\label{sec:introduction}
% Section Road Map:
% Motivation
%   1. Neural networks and deep learning are important and have many applications.
%   2. Training neural networks is time consuming due to evergrowing models (parameters) and datasets
%      (number and size of inputs (e.g. images)), and there is reason to speed this up.
%   3. Parallelizing and distributing the training is a/the solution to this.
%
% Contributions
%   1. Comparision of multiple parallelism approaches for distributed deep learning
%   2. Implementation of one such technique (synchronous SGD with all-reduce). Evaluation on image classicication datasets.
%   3. Results: TODO: main findings
%
% Paper Road Map
% 	1. Short road map containing the remaining sections

%% Motivation
Neural networks have become an essential factor in solving many practical challenges.
There is a multitude of applications from computer vision, audio and speech processing, to processing text and natural language \cite{chen2016-Revisiting-distributed-synchronous-SGD,dean2012-Large-scale-distributed,yadan2013-Multi-GPU-Training}.
More practical applications include machine translation \cite{sutskever2014sequence}, self-driving cars, and automatic driver assistance systems \cite{jin2016-How-to-scale}.
%
In this paper, we focus mostly on the convolutional neural networks (CNNs) used in computer vision.
Typical tasks include image classification, character and object recognition, and object detection \cite{paine2013-GPU-async-SGD}.


The recent advances in deep learning are facilitated by a significant increase in both model size and available training data, accompanied further by the increased computing power \cite{chen2016-Revisiting-distributed-synchronous-SGD,chilimbi2014-Project-Adam}.
%
While datasets used to be rather small contain tens of thousands of images \cite{lecun1998gradient,krizhevsky2009CIFAR10}, recent datasets like ImageNet \cite{imagenet_cvpr09} contain millions of images for thousands of classes.
Moreover, the internal datasets of some companies are said to contain billions of images with hundreds of thousands of classes \cite{iandola2016-Firecaffe}.
%
To accommodate these massive datasets models with larger learning capacities are required \cite{krizhevsky2012-AlexNet}.
For example, deep learning significantly increases the number of layers in a neural network, which in turn multiplies the number of model parameters.
In general, larger models with more parameters can yield better performance than smaller networks \cite{coates2013-DL-COTS-HPC,dean2012-Large-scale-distributed,krizhevsky2012-AlexNet}.


Increasing the number of training examples and model parameters can improve the resulting accuracy significantly \cite{dean2012-Large-scale-distributed}
However, all those advances lead to substantially long training times of multiple days or even weeks \cite{jin2016-How-to-scale,chilimbi2014-Project-Adam}.
This makes experiments difficult and time-consuming \cite{yadan2013-Multi-GPU-Training,jiang2018-FiLayer} and is often a bottleneck to developing new, high accuracy model architectures \cite{iandola2016-Firecaffe}.

Consequently, parallelizing and distributing the training is necessary further to utilize all available computational power \cite{jiang2018-FiLayer,recht2011-Hogwild}.
Additionally, their sheer size prevents some datasets and models from fitting into the memory of a single machine \cite{recht2011-Hogwild,dean2012-Large-scale-distributed}.
This is especially prevalent when training on GPUs, as the available memory is much smaller than on CPUs \cite{dean2012-Large-scale-distributed}.
Circumventing memory bottlenecks can thus be an additional advantage when using distributed training.
% TODO: warum sind bisherige Ansätze (z.B. in PyTorch/Tensorflow) nicht gut genug?

% Hypthesis and overview over results/contributions
Different strategies for parallel and distributed training of neural networks have been developed.
Most techniques apply model parallelism, data parallelism, or hybrid approaches combining both approaches.
This paper discusses these different variants in detail, comparing their assets and drawbacks, and when to use which approach.
Additionally, we implement data parallel training using synchronous communication with all-reduce.
We test this training using convolutional neural networks on the well-known image classification problems MNIST and CIFAR10.
TODO: short summary of results/conclusion % TODO: something about the results from section 6/7

%% Organisation of this paper
This seminar paper is organized as follows:
The next section gives a short introduction to neural networks and their training.
After that, \autoref{sec:related_work} gives an overview of previous and related work on parallelizing the training of neural networks.
The different parallelization techniques are covered in detail in \autoref{sec:parallelizing_neural_networks}.
We discuss both model and data parallelism as well as hybrid approaches.
\autoref{sec:implementation} describes the implementation of data-parallel training using synchronous stochastic gradient descent (SGD).
The results are illustrated in \autoref{sec:experimental_results}.
Finally, \autoref{sec:conclusion} summarizes our contributions and results and gives an outlook on possible future work.

% section introduction (end)

\section{Preliminaries} % (fold)
\label{sec:preliminaries}
This section gives a short introduction to machine learning using convolutional neural networks, defining all necessary concepts used in the remainder of this paper.
More in-depth information can, for example, be found in \cite{goodfellow2016-DeepLearningBook}.

A dataset typically consists of two separate sets of images, one for training the network and one to test its accuracy on unseen inputs and to avoid overfitting.
Each of these sets contains pairs of an input instance and the correct label.
In this paper, we focus on the image classification problem, where given an image, the neural network shall correctly determine the class depicted in the image.

Neural networks consist of multiple neurons.
A neuron has some weighted connections, for example to previous neurons, with weights $w$ and a bias $b$.
It receives some input $x$ via its connections and computes a weighted sum $w^Tx+b$ which is then passed to an \emph{activation function} $\phi(\cdot)$.
Typical activation functions are $\tanh$, the logistic function, or more recently, the rectified linear unit (ReLU).
The output $\phi(w^Tx+b)$ of the neuron can then be passed as input to the next neuron.

Multiple neurons can be organized into \emph{layers} where the output of each layer is passed as input to the next.
The input data -- for example, the pixel values of a training image -- is passed to the first layer.
The network output is encoded in the last layer's output.
%
In \emph{deep learning}, the number of different layers is high.
This enables the model to learn hierarchical features from the raw input data instead of using hand-crafted features \cite{chilimbi2014-Project-Adam}.
%
There are different types of layers, depending on how the neurons are connected to the previous layer.
In computer vision and image classification, the most important types of layers are fully-connected layers, convolutional layers, and pooling-layers.

In \emph{fully-connected layers}, each neuron of the first layer is connected to each neuron in the second layer.
These layers consist of many connections which result in numerous parameters.
Fully-connected layers are typically used to determine the image class once the features have been extracted.

\emph{Convolutional layers} are based on the convolutional filtering used in digital image processing, for example, in the Gaussian blur filter or edge detection filters like Sobel or Prewitt \cite{Stockman:2001:CV:558008}.
They use prior knowledge, for example, a hierarchy in the image features \cite{lecun2010convolutional}, to reduce the number of parameters.
Convolutional layers consist of a small filter -- usually between 3x3 and 7x7 pixels -- that is shifted over the image.
At each pixel, the weighted sum of its surrounding pixels is computed and passed to the activation function.
Instead of connecting individual neurons with weighted connections, the filter weights are therefore reused for each pixel.
This reduces the number of parameters significantly, which in turn speeds up the training.
Convolutional layers are often used together with pooling layers as features extractors \cite{lecun2010convolutional}.

\emph{Pooling layers} aggregate adjacent pixels to a single output pixel using either the sum, the average or the maximum of the pooled pixels.
Similarly to convolutional layers, the pooling operation is shifted over the image and repeatedly applied to the different parts of the image.
Pooling layers reduce the spatial size of the input image and generally improve the model's robustness to noise and clutter in the input \cite{lecun2010convolutional,chilimbi2014-Project-Adam}.

A \emph{loss function} is used to evaluate the accuracy by comparing the network's output to the actual, expected output.
Different loss functions for different types of problems exist.
For image classification, a typical loss function is the cross-entropy loss.
%
In general, neural networks can be thought of as ``large parameterized function[s]'' \cite{paine2013-GPU-async-SGD} where all the weights and biases are model parameters that can be modified to improve the accuracy.
When training a neural network, we aim to minimize the loss by adjusting its parameters.

A widely used optimization strategy for training neural networks is \emph{stochastic gradient descent (SGD)}. % TODO cite?
Gradient descent is an optimization algorithm to find local minima by adjusting the parameters in the opposite direction of the gradient.
%
Usually, \emph{backpropagation} is used to compute the gradient of a neural network.
Backpropagation is based on the chain rule; a rule used to compute the derivative of function compositions.
%
Instead of computing the gradient over the entire training set at once, SGD partitions the shuffled training set into \emph{batches}.
For each batch, we compute an averaged gradient using backpropagation and update the model parameters accordingly.
%
A \emph{training epoch} consists of processing each batch in the training set once.
Usually, multiple epochs -- that is multiple passes through the training set -- are required to train a neural network to convergence \cite{chilimbi2014-Project-Adam}.

There are many different hyper-parameters to tune the neural network and its training.
In this paper, only the \emph{batch size}, that is the number of training instances per batch, is of importance.
A larger batch size generally reduces the training time per epoch while increasing the number of epochs required to reach a certain accuracy.
As we will see in \autoref{sec:parallelizing_neural_networks} and \ref{sec:experimental_results}, the batch size can also affect the scalability of some parallelization techniques.

% TODO additional? GPUs for neural networks and why they work well, short MPI introduction

% section preliminaries (end)

\section{Related Work} % (fold)
\label{sec:related_work}
A general overview of different parallelization techniques in the time between 1995 and 2011 is presented by Upadhyaya \cite{upadhyaya2013parallel}.
%
% Single-Node parallelism for computer vision networks
Single-node parallelization strategies include \emph{Hogwild} \cite{recht2011-Hogwild}, a shared memory implementation of SGD by Niu et al., which in contrast to previous approaches does not rely on locking.
The authors argue that, since many neural networks are sparse, the chance of different processors overwriting each other's progress is so small that it does not affect the convergence of the network.
%
Jiang et al. \cite{jiang2018-FiLayer} introduce \emph{FiLayer}, a single-node parallelization that can be used in addition to model and data parallelism.
Their inter-layer parallelization uses a pipeline-based processing of the different network layers, whereas their intra-layer approach parallelizes the computation in a single convolutional layer.

% Distributed parallelism for computer vision networks
An influential work on distributed training of neural networks is Google’s \emph{DistBelief} framework introduced by Dean et al. \cite{dean2012-Large-scale-distributed}.
DistBelief uses model and data parallelism for large-scale distributed training of deep neural networks.
They are able to train significantly larger models than before by utilizing tens of thousands of CPU cores.
Their \emph{Downpour SGD} is based on an asynchronous approach to data-parallelism using a central parameter server.
%
Paine et al. \cite{paine2013-GPU-async-SGD} propose a variant of DistBelief's Downpour SGD using GPUs instead of CPUs.
To reduce the overhead of GPU-CPU data-transfer, workers only communicated with the parameter server every n-th step instead of after each optimization step.

Multiple other algorithms based on a data-parallel approach have been developed.
%
For example, \emph{Elastic averaging SGD}, developed by Zhang et al. \cite{zhang2015-Elastic-AvgSGD}, reduces the communication overhead between the workers and the server compared to the standard parameter server approach.
It, furthermore, speeds up the convergence by enabling more exploration and allowing more divergence in the worker parameters.
%
Jin et al. \cite{jin2016-How-to-scale} introduce a new asynchronous, data parallel training algorithm called \emph{gossiping SGD}.
They compare gossiping SGD to both synchronous SGD using All-Reduce and asynchronous, elastic averaging SGD \cite{zhang2015-Elastic-AvgSGD}.
They conclude that asynchronous methods converge faster for large step sizes and up to 32 nodes, while synchronous SGD converges faster when using more nodes or smaller steps \cite{jin2016-How-to-scale}.
%
For synchronous SGD, Chen et al. \cite{chen2016-Revisiting-distributed-synchronous-SGD} develop an improved version which outperforms previous asynchronous approaches by mitigating the straggler effect with backup workers.
%
With \emph{FireCaffe}, Iandola et al. \cite{iandola2016-Firecaffe} present a multi-GPU implementation of data parallelism focused on reducing the communication overhead.
They compare different communication algorithms and find that reduction trees outperform the parameter server approach regarding efficiency and scalability.
Additionally, they note that a higher batch size can improve the scalability even further.

Focusing on model parallelism, Coates et al. \cite{coates2013-DL-COTS-HPC} investigate training on Commodity Off-The-Shelf High-Performance Computing (COTS HPC) systems consisting of a cluster of GPUs.
They provide several solutions to the challenges induced by this infrastructure, such as communications bottlenecks and the more complicated design necessary to arrange training on many GPUs.

Both approaches are also frequently combined to hybrid techniques.
For example, Krizhevsky \cite{krizhevsky2014-One-weird-trick} proposes to use data parallelism in the convolutional layers and model parallelism in the fully-connected layers of a single convolutional neural network.
%
Furthermore, Das et al. \cite{das2016-Distributed-deep-learning} provide an optimized implementation of single-node, synchronous SGD and extend this to a distributed, multi-node version using a hybrid approach of both model and data parallelism.
Unlike many other implementations, they do not alter hyperparameters such as the minibatch size or learning rate.

% Speech DNNs
There have also been several attempts at parallelizing deep neural networks for speech processing.
However, they generally scale significantly worse than the typical CNNs used in computer vision due to the higher amount of fully-connected layers \cite{dean2012-Large-scale-distributed,seide2014-Speech-DNNs}.
Seide et al. \cite{seide2014-Speech-DNNs,seide2014-1-bit-stochastic} investigate parallelizing these types of networks using both model and data parallelism.
They focus on adjusting the learning rate and minibatch size \cite{seide2014-Speech-DNNs} and reducing the amount of communicated data \cite{seide2014-1-bit-stochastic} to improve the scalability.
Nevertheless, they conclude that any ``dramatic speed-ups'' for speech DNNs can only be reached by fundamentally changing and parallelizing the underlying training algorithm instead of simply applying model or data parallelism \cite{seide2014-Speech-DNNs,seide2014-1-bit-stochastic}.

% section related_work (end)

\section{Parallelizing Neural Networks} % (fold)
\label{sec:parallelizing_neural_networks}

\subsection{Model Parallelism} % (fold)
\label{sub:model_parallelism}
% definition/how does it work
% pro/contra (compared to data-parallelism)

% subsection model_parallelism (end)

\subsection{Data Parallelism} % (fold)
\label{sub:data_parallelism}
% definition/how does it work in general
% different approaches: synchronous/asynchronous etc.
% pro/contra (compared to model-parallelism)

% subsection data_parallelism (end)

\subsection{Hybrid Approaches} % (fold)
\label{sub:hybrid_approaches}
% definition/how does it work
% motivation, pro/contra to just one parallelism technique

% subsection hybrid_approaches (end)

% section parallelizing_neural_networks (end)


% for implementation: why did I choose synchronous data parallelism with all-reduce? what's the advantage for parallel programming

\section{Implementation and Methodology} % (fold)
\label{sec:implementation}
We implement and evaluate a synchronous, data-parallel training using MPI-All-Reduce to communicate and aggregate the gradients between different processes.
% TODO: reasons why this technique implemented and not e.g. model parallelism or asynch approaches.

We test our implementation with two CNNs:
\emph{LeNet5} \cite{lecun1998gradient} consists of three convolutional layers, two maximum pooling layers, and two fully-connected layers.
\emph{TorchNet}\footnote{\url{https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\#define-the-network}} consists of two convolutional layers, two maximum pooling layers, and three fully-connected layers.

These models are trained on the image classification datasets \emph{MNIST} \cite{lecun1998gradient} and \emph{CIFAR10} \cite{krizhevsky2009CIFAR10}.
MNIST contains 60,000 training images and 10,000 test images of handwritten digits.
CIFAR10 consists of 50,000 training images and 10,000 test images for ten different classes such as cat, airplane, or bird.

We use a learning rate of $0.001$ and a Nesterov momentum of $0.9$ for all experiments.
This is equal to the values used by Jin et al. in \cite{jin2016-How-to-scale}.
In contrast to some other publications, e.g. \cite{jin2016-How-to-scale}, we do not adjust the learning rate during the training.
%
The training data is divided into batches with batch sizes between $16$ and $512$.
When training in parallel, batches are divided equally amongst all processes.
Thus, the mini-batch size per process equals the batch size divided by the number of processes.
%
Each process is assigned an equal share of the training data from which it picks its mini-batches.
Between training epochs, the each process shuffles its subset of the training data.
Training images are never exchanged between processes.
It is important to note that this can reduce the variance when shuffling the data.

The training is implemented in Python v3.6.6 with PyTorch v1.1.0, Torchvision v0.3.0, and Numpy v1.16.4.
On the GPU, CUDA v10.0 is used for computation.
For communication, OpenMPI v3.1 with the python wrapper MPI4Py v3.0.2.
The datasets were converted to HDF5 format using HDF5 v1.10 (on ForHLR II) and v1.8.12 (on LSDF) and are read via H5Py v2.9.0.

We evaluated our implementation two different machines:
The \emph{LSDF} server consists of two Intel Xeon E5-2630 v3 processors running at 2.40 – 3.20~GHz with eight cores each and two threads per core.
Additionally, LSDF contains four NVIDIA Tesla K80 GPU, each offering 4992 NVIDIA CUDA cores and 24~GB of GDDR5 memory.
%
We, furthermore, used Intel Xeon E5-2660 v3 processors on the HPC-system \emph{ForHLR II}. % TODO dünne oder fette knoten?
Each processor runs at 2.60 – 3.30~GHz and consists of ten cores per processor and two threads per core.
%
Both machines use Red Hat Enterprise Linux 7 as operating system.

% TODO: interesting/relevant implementation details?

% section implementation (end)

\section{Experimental Results} % (fold)
\label{sec:experimental_results}
% illustrate results with plots etc.
% describe and interpret results

\begin{figure}[h]
\centering
\includegraphics[width=2.5in]{../evaluation/diagrams-paper/validation-loss-by-epochs--MNIST}
\caption{The validation loss after each epoch for different batch sizes using sequential training, LeNet5 and MNIST.
As expected, the smaller the batch size, the faster is the model's convergence.
However, at the same time, a larger batch size speeds up the training time per epoch as can be seen in \autoref{fig:loss_by_time}.}
\label{fig:loss_by_epoch}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=2.5in]{../evaluation/diagrams-paper/epoch-time--LSDF-GPU-MNIST}
\caption{The ratio of computation to communication time per epoch for different batch sizes using parallel training on four GPUs, LeNet5 and MNIST.
As expected, both the computation and the communication decreases the larger the batch size.
The reduction in computation time is in accordance with the results in \autoref{sub:impact_of_the_batch_size}.
Furthermore, the larger each batch, the fewer batches are in each epoch.
Since communication is required after each batch, larger batches therefore results in less communication phases in total, and thus less time spent communicating.}
\label{fig:comm_time_by_batch_size}
\end{figure}

\begin{figure*}[!t]
  \normalsize
  \centering
  \includegraphics[width=\textwidth]{../evaluation/diagrams-paper/epoch-time-bs-256--MNIST}
  \caption{The ratio of computation to communication time per epoch when training LeNet5 on MNIST using different machines and different numbers of processes.
  On LSDF CPU (Subfigure A), the behavior is as expected with the computation time decreasing and the communication time slightly increasing when using more processes.
  Interestingly, on the GPU (Subfigure B), not only the computation but also the communication time decreases when utilizing more GPUs.
  This initially counterintuitive observation might be explained by PCI-E piggybacking.
  The results on ForHLR II, see Subfigure C, show a decrease in computation time with more processes.
  However, for 8 or more processes, the communication time sharply increases.
  This might be an effect of ForHLR II's NUMA architecture.
  Nonetheless, this effect should not be as dramatic as currently measured.
  }
  \label{fig:comm_time_by_machine}
  \vspace*{4pt}
  \hrulefill
\end{figure*}

\begin{figure*}[!t]
  \normalsize
  \centering
  \includegraphics[width=\textwidth]{../evaluation/diagrams-paper/speedup--LSDF-GPU-MNIST}
  \caption{The training speedup (solid with line points) and efficiency (dashed) for LeNet5 on MNIST for different batch sizes using GPUs for training.
  Subplot A depicts the speedup (and efficiency) per epoch.
  As expected, larger batch sizes scale better due to the reduced communication.
  In Subplot B, the total training time until a certain threshold of the validation loss -- in this case $0.5$ -- is reached.
  TODO interpretation.
  }
  \label{fig:speedup}
  \vspace*{4pt}
  \hrulefill
\end{figure*}

% subsection speedup (end)

% section experimental_results (end)

\section{Conclusion} % (fold)
\label{sec:conclusion}
% summarize contributions, reference to introduction/abstract
% summarize central results and implications
% revisit section key points
% Benefits and shortcomings of approach and results

% section conclusion (end)


% \begin{thebibliography}{1}
% % TODO
% \end{thebibliography}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\appendix[Additional Experimental Results]

\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../evaluation/diagrams-paper/validation-loss-by-time-and-bs--MNIST}
\caption{The validation loss by training time for different batch sizes using sequential training, LeNet5 and MNIST.
This figure is related to \autoref{fig:loss_by_epoch} to the extent that the same data is displayed -- once per epoch and once per training time.
Displaying the loss by time demonstrates how larger batch sizes result in faster epochs but selecting too large batch sizes can compromise the convergence.}
\label{fig:loss_by_time}
\end{figure}

\end{document}

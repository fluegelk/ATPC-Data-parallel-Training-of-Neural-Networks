\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}


% configs from ba
\usepackage[english]{babel}
\usepackage{booktabs}
\usepackage[raiselinks=true,
			bookmarks=true,
			bookmarksopenlevel=1,
			bookmarksopen=true,
			bookmarksnumbered=true,
			hyperindex=true,
			plainpages=false,
			pdfpagelabels=true,
			pdfborder={0 0 0.5},
			colorlinks=false,
			linkbordercolor={0 0.61 0.50},
			citebordercolor={0 0.61 0.50}]{hyperref}
\addto\extrasenglish{%
  \def\chapterautorefname{Chapter}%
  \def\chapterrefname{Chapter}%
  \def\sectionautorefname{Section}%
  \def\sectionrefname{Section}%
  \def\subsectionautorefname{Section}%
  \def\subsectionrefname{Section}%
  \def\subsubsectionautorefname{Section}%
  \def\subsubsectionrefname{Section}%
  \def\paragraphautorefname{Paragraph}%
  \def\paragraphrefname{Paragraph}%
}

\usepackage{array}
\usepackage{adjustbox}
\usepackage{tablefootnote}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% \ifCLASSOPTIONcompsoc
%   % IEEE Computer Society needs nocompress option
%   % requires cite.sty v4.0 or later (November 2003)
%   \usepackage[nocompress]{cite}
% \else
  % normal IEEE
  \usepackage{cite}
% \fi


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newcounter{MYtempeqncnt}

\begin{document}

\title{Data Parallel Training of Neural Networks}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Katharina Fl√ºgel}
\IEEEauthorblockA{Karlsruhe Institute of Technologie (KIT)\\
katharina.fluegel@student.kit.edu}}


% make the title area
\maketitle

% General rule: no math, special symbols or citations in the abstract
\begin{abstract}
With an increase in both model and data size, training neural networks has become increasingly more time-consuming.
Techniques to parallelize and distribute the training in order to reduce these training times are thus ever more important.
This seminar paper surveys different strategies for both distribution and parallelization when training neural networks.
Two essential principles to distinguish different parallelization approaches are model and data parallelism.
While we describe the general concept of model parallelism, this paper focuses mostly on data parallelism.
We illustrate multiple data parallel algorithms, including both synchronous and asynchronous communication schemes.
%
We have implemented synchronous stochastic gradient descend and evaluate it by training convolutional neural networks on image classification datasets.
With 16 CPU cores, we achieve speedups of up to 7.70 for the training time per epoch.
At the same time distributed training decreases the convergences yielding an effective speedup of 6.23.

\end{abstract}

\section{Introduction} % (fold)
\label{sec:introduction}
% Section Road Map:
% Motivation
%   1. Neural networks and deep learning are important and have many applications.
%   2. Training neural networks is time consuming due to evergrowing models (parameters) and datasets
%      (number and size of inputs (e.g. images)), and there is reason to speed this up.
%   3. Parallelizing and distributing the training is a/the solution to this.
%
% Contributions
%   1. Comparision of multiple parallelism approaches for distributed deep learning
%   2. Implementation of one such technique (synchronous SGD with all-reduce). Evaluation on image classicication datasets.
%   3. Results: TODO: main findings
%
% Paper Road Map
% 	1. Short road map containing the remaining sections

%% Motivation
Neural networks have become an essential factor in solving many practical challenges.
There is a multitude of applications from computer vision, audio and speech processing, to processing text and natural languages~\cite{chen2016-Revisiting-distributed-synchronous-SGD,dean2012-Large-scale-distributed,yadan2013-Multi-GPU-Training}.
More practical applications include machine translation~\cite{sutskever2014sequence}, self-driving cars, and automatic driver assistance systems~\cite{jin2016-How-to-scale}.
%
In this paper, we focus mostly on the convolutional neural networks (CNNs) used in computer vision.
Typical tasks include image classification, character and object recognition, and object detection~\cite{paine2013-GPU-async-SGD}.


The recent advances in deep learning are facilitated by a significant increase in both model size and available training data, accompanied further by the increased computational power~\cite{chen2016-Revisiting-distributed-synchronous-SGD,chilimbi2014-Project-Adam}.
%
While datasets used to be rather small, containing tens of thousands of images~\cite{lecun1998gradient,krizhevsky2009CIFAR10}, recent datasets like ImageNet~\cite{imagenet_cvpr09} contain millions of images for thousands of classes.
Moreover, the internal datasets of some companies are said to contain billions of images with hundreds of thousands of classes~\cite{iandola2016-Firecaffe}, and self-driving cars collect almost one gigabyte per second~\cite{jin2016-How-to-scale}.
%
To accommodate these massive datasets, models with larger learning capacities are required~\cite{krizhevsky2012-AlexNet}.
For example, deep learning significantly increases the number of layers in a neural network, which in turn multiplies the number of model parameters.
In general, larger models with more parameters can yield better performance than smaller networks~\cite{coates2013-DL-COTS-HPC,dean2012-Large-scale-distributed,krizhevsky2012-AlexNet}.


Increasing the number of training examples and model parameters can improve the resulting accuracy significantly~\cite{dean2012-Large-scale-distributed}.
However, all those advances lead to substantially long training times of multiple days or even weeks~\cite{jin2016-How-to-scale,chilimbi2014-Project-Adam,sergeev2018horovod}.
This makes experiments difficult and time-consuming~\cite{yadan2013-Multi-GPU-Training,jiang2018-FiLayer} and is often a bottleneck to developing new, high accuracy model architectures~\cite{iandola2016-Firecaffe}.

Consequently, parallelizing and distributing the training is necessary to utilize all available computational power further~\cite{jiang2018-FiLayer,recht2011-Hogwild}.
Additionally, their sheer size prevents some datasets and models from fitting into the memory of a single machine~\cite{recht2011-Hogwild,dean2012-Large-scale-distributed}.
This is especially prevalent when training on GPUs, as the available memory is much smaller than on CPUs~\cite{dean2012-Large-scale-distributed}.
Circumventing memory bottlenecks can thus be an additional advantage when using distributed training.

% Hypthesis and overview over results/contributions
Different strategies for parallel and distributed training of neural networks have been developed.
Most techniques apply model parallelism, data parallelism, or hybrid approaches combining both methods.
This paper discusses these different variants in detail, comparing their advantages and drawbacks, and when to use which approach.
Additionally, we implement data parallel training using synchronous communication to average the gradients.
We test this training using convolutional neural networks like LeNet5~\cite{lecun1998gradient} on the well-known image classification problems MNIST~\cite{lecun1998gradient} and \mbox{CIFAR10}~\cite{krizhevsky2009CIFAR10}.
Using LeNet5, we achieve training speedups of up to $3.21$ on four GPUs and $7.70$ on 16 CPUs.
We observe several interesting communication effects that might be attributed to GPU-CPU-communication, PCI-E piggybacking, and NUMA architecture.

%% Organisation of this paper
This seminar paper is organized as follows.
The next section gives a short introduction to neural networks and their training.
After that, \autoref{sec:related_work} gives an overview of previous and related work on parallelizing the training of neural networks.
The different parallelization techniques are covered in detail in \autoref{sec:parallelizing_neural_networks}, where we discuss both model and data parallelism.
\autoref{sec:implementation} describes the implementation of a data parallel training using synchronous stochastic gradient descent (SGD).
The results are illustrated in \autoref{sec:experimental_results}.
Finally, \autoref{sec:conclusion} summarizes our results and gives an outlook on possible future work.

% section introduction (end)

\section{Preliminaries} % (fold)
\label{sec:preliminaries}
This section gives a short introduction to machine learning using convolutional neural networks, defining all necessary concepts used in the remainder of this paper.
More in-depth information can, for example, be found in~\cite{goodfellow2016-DeepLearningBook}.
Additionally, we give a brief overview of GPU-accelerated training of neural networks and the Message Passing Interface (MPI).

A dataset for supervised training typically consists of two separate sets of images, one for training the model and one to test its generalization to unseen inputs to avoid overfitting.
Each of these sets contains pairs of an input instance and the associated labels.
In this paper, we focus on the image classification problem, where given an image, the neural network shall correctly determine the class depicted in the image.
Training pairs, thus, consist of an image and the associated class.
Consequently, we use the terms ``label'' and ``class'' synonymously in the following.
When training a model, we adjust model such that it assigns the correct labels to an input image.

Neural networks consist of multiple neurons.
A neuron has some weighted connections, for example to previous neurons, with weights $w$ and a bias $b$.
It receives some input $x$ via its connections and computes a weighted sum $w^Tx+b$ which is then passed to an \emph{activation function} $\phi(\cdot)$.
Typical activation functions are $\tanh$, the logistic function, or more recently, the rectified linear unit (ReLU).
The output $\phi(w^Tx+b)$ of the neuron can then be passed as input to the next neuron.

Multiple neurons can be organized into \emph{layers} where the output of each layer is passed as input to the next.
The input data---for example, the pixel values of a training image---is passed to the first layer.
The network output is encoded in the last layer's output.
%
In \emph{deep learning}, the number of different layers is high.
This enables the model to learn hierarchical features from the raw input data instead of using hand-crafted features~\cite{chilimbi2014-Project-Adam}.
%
There are different types of layers, depending on how the neurons are connected to the previous layer.
In computer vision and image classification, the most important types of layers are fully-connected layers, convolutional layers, and pooling layers.

In \emph{fully-connected layers}, each neuron of the first layer is connected to each neuron in the second layer.
These layers consist---as their name suggests---of a fully-connected mesh of connections which results in a substantial amount parameters.
Fully-connected layers are typically used as final layers of the model, predicting the actual class once the features have been extracted.

\emph{Convolutional layers} are based on the convolutional filtering used in digital image processing, for example, in the Gaussian blur filter or edge detection filters like Sobel and Prewitt~\cite{Stockman:2001:CV:558008}.
% They use prior knowledge, for example, a hierarchy in the image features~\cite{lecun2010convolutional}, to reduce the number of parameters.
Convolutional layers consist of a small filter---usually between 3x3 and 7x7 pixels---that is shifted over the image.
At each pixel, the weighted sum of its surrounding pixels and the kernel is computed and passed to the activation function.
Instead of connecting individual neurons with weighted connections, the kernel weights are therefore reused for all pixels.
This reduces the number of parameters significantly, which in turn speeds up the training.
Convolutional layers are often used as features extractors together with pooling layers~\cite{lecun2010convolutional}.

\emph{Pooling layers} aggregate adjacent pixels to a single output pixel using either the sum, the average or the maximum of the pooled pixels.
Similarly to convolutional layers, the pooling operation is shifted over the image and repeatedly applied to the different parts of the image.
Pooling layers reduce the spatial size of the input image and generally improve the model's robustness to noise and clutter in the input~\cite{lecun2010convolutional,chilimbi2014-Project-Adam}.

A \emph{loss function} is used to evaluate the accuracy of the model by comparing the network's output to the actual, expected output.
Different loss functions for different types of problems exist.
For image classification, a typical loss function is the cross-entropy loss.
%
In general, neural networks can be thought of as ``large parameterized function[s]''~\cite{paine2013-GPU-async-SGD} where all the weights and biases are model parameters that can be modified to improve the accuracy.
When training a neural network, we aim to minimize the loss by adjusting its parameters.

A widely used optimization strategy for training neural networks is \emph{stochastic gradient descent (SGD)}. % TODO cite?
Gradient descent is an optimization algorithm to find local minima by adjusting the parameters in the opposite direction of the gradient.
%
Usually, \emph{backpropagation} is used to compute the gradient of a neural network.
Backpropagation is based on the chain rule; a rule used to compute the derivative of function compositions.
%
Instead of computing the gradient over the entire training set at once, SGD partitions the shuffled training set into \emph{batches}.
For each batch, we compute an averaged gradient using backpropagation and update the model parameters accordingly.
%
A \emph{training epoch} consists of processing each batch in the training set once.
Usually, multiple epochs---that is multiple passes through the training set---are required to train a neural network to convergence~\cite{chilimbi2014-Project-Adam}.

There are many different hyper-parameters to tune the neural network and its training.
In this paper, only the \emph{batch size}, that is the number of training instances per batch, is of importance.
A larger batch size generally reduces the training time per epoch while increasing the number of epochs required to reach a certain accuracy.
As we will see in \autoref{sec:parallelizing_neural_networks} and \ref{sec:experimental_results}, the batch size can also affect the scalability of some parallelization techniques.

Nowadays, many implementations of neural networks utilize the processing power of \emph{graphic processing units (GPUs)} to speed up the training.
GPUs are heavily parallel and use more pipelining than CPUs~\cite{oh2004gpu}.
In general, they perform in SIMD mode---that is, applying a single instruction to multiple inputs.
This makes GPUs very suited for vector processing.
%
When training neural networks, instead of computing the weighted sums individually for each neuron, multiple neurons can be processed together using matrix multiplication, which can then be computed very efficiently on the GPU~\cite{oh2004gpu}.
Training neural networks on a GPU is thus often orders of magnitude faster than on a CPU~\cite{oh2004gpu,strigl2010performance}.

The \emph{MPI message passing interface}~\cite{walker1996mpi} defines a set of operations to communicate between different processes by sending messages.
This includes blocking and non-blocking point-to-point communication and multiple collective operations for groups of processes.
In this paper, we require only the all-reduce operation.
Each process in the communication group holds some value to be aggregated.
All-reduce collects these value, aggregates them using, for example, a sum function, and distributes the aggregated value to all participating processes.

% section preliminaries (end)

\section{Related Work} % (fold)
\label{sec:related_work}
A general overview of different parallelization techniques in the time between 1995 and 2011 is presented by Upadhyaya~\cite{upadhyaya2013parallel}.
%
% Single-Node parallelism for computer vision networks
Single-node parallelization strategies include \emph{Hogwild}~\cite{recht2011-Hogwild}, a shared memory implementation of SGD by Niu et al., which in contrast to previous approaches does not rely on locking.
The authors argue that, since many neural networks are sparse, the chance of different processors overwriting each other's progress is so small that it does not affect the convergence of the network.
%
Jiang et al.~\cite{jiang2018-FiLayer} introduce \emph{FiLayer}, a single-node parallelization that can be used in addition to model and data parallelism.
Their inter-layer parallelization uses a pipeline-based processing of the different network layers, whereas their intra-layer approach parallelizes the computation of a single convolutional layer.

% Distributed parallelism for computer vision networks
An influential work on distributed training of neural networks is Google‚Äôs \emph{DistBelief} framework introduced by Dean et al.~\cite{dean2012-Large-scale-distributed}.
DistBelief uses model and data parallelism for large-scale distributed training of deep neural networks.
They are able to train significantly larger models than before by utilizing tens of thousands of CPU cores.
Their \emph{Downpour SGD} is based on an asynchronous approach to data parallelism using a central parameter server.
%
Paine et al.~\cite{paine2013-GPU-async-SGD} propose a variant of DistBelief's Downpour SGD using GPUs instead of CPUs.
To reduce the overhead of GPU-CPU data-transfer, workers only communicated with the parameter server every n-th step instead of after each optimization step.

Multiple other algorithms based on a data parallel approach have been developed.
%
For example, \emph{Elastic averaging SGD}, developed by Zhang et al.~\cite{zhang2015-Elastic-AvgSGD}, reduces the communication overhead between the workers and the server compared to the standard parameter server approach.
It, furthermore, speeds up the convergence by enabling more exploration and allowing more divergence in the worker parameters.
%
Jin et al.~\cite{jin2016-How-to-scale} introduce a new asynchronous, data parallel training algorithm called \emph{gossiping SGD}.
They compare gossiping SGD to both synchronous SGD using all-reduce and asynchronous, elastic averaging SGD~\cite{zhang2015-Elastic-AvgSGD}.
They conclude that asynchronous methods converge faster for large step sizes and up to 32 nodes, while synchronous SGD converges faster when using more nodes or smaller steps~\cite{jin2016-How-to-scale}.
%
For synchronous SGD, Chen et al.~\cite{chen2016-Revisiting-distributed-synchronous-SGD} develop an improved version which outperforms previous asynchronous approaches by mitigating the straggler effect with backup workers.
%
With \emph{FireCaffe}, Iandola et al.~\cite{iandola2016-Firecaffe} present a multi-GPU implementation of data parallelism focused on reducing the communication overhead.
They compare different communication algorithms and find that reduction trees outperform the parameter server approach regarding efficiency and scalability.
Additionally, they note that a larger batch size can improve the scalability even further.

Focusing on model parallelism, Coates et al.~\cite{coates2013-DL-COTS-HPC} investigate training on Commodity Off-The-Shelf High-Performance Computing (COTS HPC) systems consisting of a cluster of GPUs.
They provide several solutions to the challenges induced by this infrastructure, such as communications bottlenecks and the more complicated design necessary to arrange training on many GPUs.

Both model and data parallelism are also frequently combined to hybrid techniques.
For example, Krizhevsky~\cite{krizhevsky2014-One-weird-trick} proposes to use data parallelism in the convolutional layers and model parallelism in the fully-connected layers of a single convolutional neural network.
%
Furthermore, Das et al.~\cite{das2016-Distributed-deep-learning} provide an optimized implementation of single-node, synchronous SGD and extend this to a distributed, multi-node version using a hybrid approach of both model and data parallelism.
Unlike many other implementations, they do not alter hyper-parameters such as the mini-batch size or learning rate.

% Speech DNNs
There have also been several attempts at parallelizing deep neural networks for speech processing.
However, they generally scale significantly worse than the typical CNNs used in computer vision due to the higher amount of fully-connected layers~\cite{dean2012-Large-scale-distributed,seide2014-Speech-DNNs}.
Seide et al.~\cite{seide2014-Speech-DNNs,seide2014-1-bit-stochastic} investigate parallelizing these types of networks using both model and data parallelism.
They focus on adjusting the learning rate and mini-batch size~\cite{seide2014-Speech-DNNs} and reducing the amount of communicated data~\cite{seide2014-1-bit-stochastic} to improve the scalability.
Nevertheless, they conclude that any ``dramatic speed-ups'' for speech DNNs can only be reached by fundamentally changing and parallelizing the underlying training algorithm instead of merely applying model or data parallelism~\cite{seide2014-Speech-DNNs,seide2014-1-bit-stochastic}.

% section related_work (end)

\section{Parallelizing Neural Networks} % (fold)
\label{sec:parallelizing_neural_networks}
This section gives an overview of the different algorithms used for distributed training of neural networks.
There are two main parallelization approaches for neural networks: model and data parallelism.
Data parallelism can be divided further into synchronous and asynchronous approaches.
Moreover, model and data parallelism can be combined to hybrid approaches to utilize both techniques as demonstrated, for example, by~\cite{krizhevsky2014-One-weird-trick,das2016-Distributed-deep-learning}.

\subsection{Model Parallelism} % (fold)
\label{sub:model_parallelism}
% definition/how does it work
% pro/contra (compared to data parallelism)

% Definition/how it works
In model parallelism, the neural network is partitioned into multiple components.
These model components can then be distributed across multiple machines and trained in parallel.
They are linked by the connections between neurons in different components.
During training, the output from one component is communicated to the next along these connections~\cite{dean2012-Large-scale-distributed}.
\autoref{fig:model_parallelism} by Dean et al.~\cite{dean2012-Large-scale-distributed} gives an example of this process.

\begin{figure}[ht]
\centering
\includegraphics[width=2.5in]{images/model-parallelism}
\caption{
A schematic illustration by Dean et al.~\cite{dean2012-Large-scale-distributed} of partitioning a neural network to distribute it for model parallelism.
The connections printed in bold cross between two model parts.
These are the connections, along which communication is required.
}
\label{fig:model_parallelism}
\end{figure}

Since we communicate neuron outputs, model parallelism generally scales best when the amount of computation per neuron is high~\cite{krizhevsky2014-One-weird-trick}.
This is typically the case in fully-connected layers~\cite{krizhevsky2014-One-weird-trick}.
However, fully-connected layers generally require more communication, as they produce more connections between the model components than models with more ``local connectivity structures''~\cite{dean2012-Large-scale-distributed}.

% Advantages
By using more machines---and thus more cores and more memory---model parallelism can substantially reduce the training time.
For example, Dean et al. were able to train large models with significant speedups using up to 144 partitions~\cite{dean2012-Large-scale-distributed}.
%
Additionally, model parallelism reduces the required memory per machine~\cite{dean2012-Large-scale-distributed}.
The model, including all its parameters, is partitioned and distributed over multiple machines.
Thus, each machine only needs to store its assigned part of the model and the corresponding parameters.
%
This can be a crucial advantage as some models are too large to fit in the memory of a single machine.
It is especially beneficial on GPUs since they typically offer significantly less memory than CPUs and transferring data from the CPU can pose a severe bottleneck~\cite{dean2012-Large-scale-distributed}.
The available memory is thus often a limiting factor to the model size on GPUs~\cite{dean2012-Large-scale-distributed,chilimbi2014-Project-Adam}.
Using model parallelism, this restriction can be mitigated, and larger models can be trained using multiple GPUs.

There are, however, also some challenges to model parallelism:
If the required time differs greatly between the machines, faster machines might need to wait a lot for slower machines, which can decrease the obtainable speedup~\cite{dean2012-Large-scale-distributed}.
Additionally, the communication time can introduce a significant overhead, especially if the number of connections between the model components is high.

% subsection model_parallelism (end)

\subsection{Data Parallelism} % (fold)
\label{sub:data_parallelism}
% definition/how does it work in general
% different approaches: synchronous/asynchronous etc.
% pro/contra (compared to model parallelism)

In data parallelism, each machine has its own copy of the neural network.
Instead of partitioning the model, data parallelism partitions the training data and distributes it across different machines.
Each machine then processes its assigned training instances and computes local gradients.
To keep the model copies consistent over all machines, the gradients are communicated between the processes.
Instead of communicating and aggregating the gradients, it is also possible to first apply local parameter updates, followed by parameter synchronization between the processes.
%
Data parallelism is generally more efficient if the computation per weight is high, as the model parameters (or gradients) are communicated.
High computation per parameter is very typical for convolutional layers where the kernel weights are reused for all input pixels.
%
An important hyper-parameter for data parallelism is the batch size.
Since the gradients are usually communicated once per batch, we can reduce the required communication by decreasing the number of batches, that is increasing the batch size.
According to Krizhevsky, data parallelism can be ``arbitrarily efficient'' with a large enough batch size~\cite{krizhevsky2014-One-weird-trick}.

There are several different algorithms to synchronize the model parameters and communicate the gradients.
Depending on the communication pattern, they can be divided into synchronous or asynchronous techniques.
Both approaches are described in the following.

\subsubsection{Synchronous Data Parallelism} % (fold)
\label{ssub:synchronous_data_parallelism}
Synchronous approaches---using, for example, synchronous SGD---usually consist of two phases alternated in lockstep~\cite{jin2016-How-to-scale}.
In the first phase, each machine processes its assigned training instances and computes a local gradient.
In the second phase, these local gradients are then aggregated using synchronous communication.
Typically, this communication is performed via an MPI all-reduce operation, which collects all local gradients and re-distributes an aggregated gradient to all processes.

This communication pattern introduces a synchronization barrier in each second phase, where all processes have to wait for each other.
As stated by Jin et al., this synchronization barrier actually makes up the majority of the communication overhead on modern networks~\cite{jin2016-How-to-scale}.
It also introduces several problems:
Synchronous SGDs are prone to a straggler effect meaning ``the slowest nodes prevent the rest from making progress''~\cite{jin2016-How-to-scale}.
% They are also ``not robust in the face of failing processors or nodes''~\cite{jin2016-How-to-scale} since the all-reduce operation cannot complete with any missing processes.
Additionally, synchronous SGDs might under-utilize the compute-resources when much of the training time is spent waiting for other processes~\cite{jin2016-How-to-scale}.

Due to the aforementioned disadvantages, synchronous SGD is often believed to be inferior to asynchronous approaches~\cite{chen2016-Revisiting-distributed-synchronous-SGD}.
However, some recent developments demonstrate synchronous SGDs outperforming asynchronous algorithms:
For example, Jin et al. find that on more than $32$ nodes synchronous SGDs scales better than their asynchronous Gossiping-SGD~\cite{jin2016-How-to-scale}.
Chen et al. demonstrate their improved synchronous SGDs with additional backup workers offers faster convergence with better accuracy than the compared asynchronous training~\cite{chen2016-Revisiting-distributed-synchronous-SGD}.

% subsubsection synchronous_data_parallelism (end)

\subsubsection{Asynchronous Data Parallelism} % (fold)
\label{ssub:asynchronous_data_parallelism}

Asynchronous approaches typically try to avoid the synchronization barrier inherent to synchronous SGD.
%
Many asynchronous techniques use a global parameter server approach.
These techniques use two types of processes: a parameter server and multiple worker processes.
The \emph{parameters server} contains the most recent consensus of the model parameters.
Multiple \emph{workers} process the training data in parallel and compute local gradients.
However, instead of communicating directly with other workers, the local gradients are sent to the parameter server, which receives them and updates its model parameters accordingly.
The worker processes periodically fetch the latest parameters from the server to update their copy of the model.
This process is also illustrated in \autoref{fig:parameter_server} by Dean et al.~\cite{dean2012-Large-scale-distributed}.

\begin{figure}[ht]
\centering
\includegraphics[width=2.5in]{images/parameter-server.pdf}
\caption{
A schematic illustration of a parameter server by Dean et al.~\cite{dean2012-Large-scale-distributed}.
The parameter server receives gradients $\delta w$ from the workers and applies them to its model parameters.
Workers can fetch parameters $w$ from the server to update their local model replicas.
Each worker has its own share of the training data---labeled as data shards in this diagram---which it uses to compute the local gradients.
}
\label{fig:parameter_server}
\end{figure}

There are different approaches at which times communication takes place.
The simplest approach, as described by Dean et al., is sending a gradient and fetching parameters after every mini-batch---that is, the subset of a batch assigned to this worker~\cite{dean2012-Large-scale-distributed}.
They also suggest reducing the commutation by increasing this interval to multiple batches.
The interval for sending gradients and fetching parameters are not required to be equal~\cite{dean2012-Large-scale-distributed}.

Since the workers only communicate with the parameter server and never with other workers, they are independent of each other.
There is, thus, no synchronization barrier.
However, asynchronous gradient descent can have its own problems:
Since all workers communicate all-to-one with the parameter server, a communication bottleneck at the parameter server can occur, while other parts of the network might be underutilized~\cite{jin2016-How-to-scale}.
Furthermore, the convergence can be worse than for synchronous approaches since the workers usually have a slightly outdated copy of the model~\cite{jin2016-How-to-scale,chen2016-Revisiting-distributed-synchronous-SGD}.
This effect is amplified when the communication delay is increased.

Multiple different variations of the classical parameter server approach have been developed, such as Elastic Averaging SGD by Zhang et al.~\cite{zhang2015-Elastic-AvgSGD} and Gossiping SGD by Jin et al.~\cite{jin2016-How-to-scale}.
There are also approaches eliminating the global parameter server altogether, like Sergeev et al., who utilize ring-all-reduce operations in their Horovod library to overcome the need for a parameter server~\cite{sergeev2018horovod}.

% subsubsection asynchronous_data_parallelism (end)

% subsection data_parallelism (end)

% section parallelizing_neural_networks (end)


% for implementation: why did I choose synchronous data parallelism with all-reduce? what's the advantage for parallel programming

\section{Implementation} % (fold)
\label{sec:implementation}
We implemented and evaluated a synchronous, data parallel training using MPI all-reduce to communicate and aggregate the gradients between different processes.
Synchronous SGDs can scale comparably well or even better than asynchronous approaches as demonstrated by~\cite{jin2016-How-to-scale,chen2016-Revisiting-distributed-synchronous-SGD} and are additionally significantly easier to implement.

The training is implemented in Python v3.6.6 with PyTorch v1.1.0~\cite{paszke2017automatic}, Torchvision v0.3.0, and Numpy v1.16.4.
On the GPU, CUDA v10.0 is used for computation.
For communication, we use OpenMPI v3.1 with the python wrapper MPI4Py v3.0.2.
The datasets were converted to HDF5 format using HDF5 v1.10 (on ForHLR II) and v1.8.12 (on LSDF) and are read via H5Py v2.9.0.
The code is available at \url{https://github.com/fluegelk/ATPC-Data-parallel-Training-of-Neural-Networks}.

We evaluated our implementation on two different machines:
The \emph{LSDF} server consists of two Intel Xeon E5-2630 v3 processors running at 3.20~GHz with eight cores each and two threads per core.
Additionally, LSDF contains four NVIDIA Tesla K80 GPU, each offering 4992 NVIDIA CUDA cores and 24~GB of GDDR5 memory.
%
We, furthermore, used Intel Xeon E5-2660 v3 processors on the HPC-system \emph{ForHLR II}
\footnote{A more detailed description of ForHLR II is given at \url{https://www.scc.kit.edu/dienste/forhlr2.php} (only available in German, accessed at 2019-07-31 9:15 UTC).}.
Each processor runs at 3.30~GHz and consists of ten cores per processor and two threads per core.
%
Both machines use Red Hat Enterprise Linux 7 as operating system.

% section implementation (end)

\section{Experimental Results} % (fold)
\label{sec:experimental_results}

This section gives an overview of our experimental results.
The models, datasets, and relevant hyper-parameters are given in \autoref{sub:methodology}.
We first discuss how changing the batch size hyper-parameter affects the training in \autoref{sub:impact_of_the_batch_size}.
In \autoref{sub:communication_time}, we compare the computation and communication times.
%for different batch sizes, on different machines, and using different numbers of processes.
Finally, \autoref{sub:speedup} examines the speedups reached by the implementation.

\subsection{Methodology} % (fold)
\label{sub:methodology}

We test our implementation with two CNNs:
\emph{LeNet5}~\cite{lecun1998gradient} consists of three convolutional layers, two maximum pooling layers, and two fully-connected layers.
\emph{TorchNet}\footnote{See \url{https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\#define-the-network} (accessed at 2019-07-31 9:15 UTC)} consists of two convolutional layers, two maximum pooling layers, and three fully-connected layers.

These models are trained on the image classification datasets \emph{MNIST}~\cite{lecun1998gradient} and \emph{CIFAR10}~\cite{krizhevsky2009CIFAR10}.
MNIST contains 60,000 training images and 10,000 test images of handwritten digits.
CIFAR10 consists of 50,000 training images and 10,000 test images for ten different classes such as cat, airplane, or bird.

We use a learning rate of $0.001$ and a Nesterov momentum of $0.9$ for all experiments.
This is equal to the values used by Jin et al. in~\cite{jin2016-How-to-scale}.
In contrast to some other publications, like~\cite{jin2016-How-to-scale}, we do not adjust the learning rate during the training.
%
The training data is divided into batches with batch sizes between $16$ and $512$.
When training in parallel, batches are divided equally amongst all processes.
Thus, the mini-batch size per process equals the batch size divided by the number of processes.
%
Each process is assigned an equal share of the training data from which it picks its mini-batches.
Between training epochs, each process shuffles its subset of the training data.
Training images are never exchanged between processes.
It is important to note that this can reduce the variance when shuffling the data.

The model is trained for a maximum of $20$ (MNIST) or $100$ (CIFAR10) epochs.
The training is stopped if either the maximum number of epochs is reached or if the validation loss could not be improved in the last ten epochs.

After each training epoch, the validation loss is measured.
To compare total training times and speedups, we consider all epochs until the validation loss drops below a certain threshold.
We select a threshold of $0.5$ for MNIST and $1.5$ for CIFAR10.
This threshold is determined by rounding up the maximum of the minimal validation loss for each training on that dataset.

% subsection methodology (end)

\subsection{Impact of the batch size} % (fold)
\label{sub:impact_of_the_batch_size}
As mentioned before, the batch size is an important hyper-parameter for data parallel training algorithms.
In general, the larger the batch size, the more epochs are required to reach convergence.
Larger batch sizes reduce the loss improvement per epoch, as the gradients are averaged across more training images, whereas with small batch sizes, many small adjustments can be made.
%
This effect is demonstrated in \autoref{fig:loss_by_epoch}.
While smaller batch sizes achieve a good validation loss of $0.5$ or less within the first few epochs, the loss curve flattens the larger the batch size.

\begin{figure}[ht]
\centering
\includegraphics[width=2.5in]{../evaluation/diagrams-paper/validation-loss-by-epochs--MNIST}
\caption{The validation loss after each epoch for different batch sizes using sequential training, LeNet5, and MNIST.
As expected, the smaller the batch size, the fewer epochs are required for convergence.}
\label{fig:loss_by_epoch}
\end{figure}

While the convergence is reduced for larger batch sizes, they also speed up the training per epoch, as there are fewer optimization steps in each epoch.
This leads to a trade-off between a greater improvement per epoch on the one hand, and faster epochs, on the other hand.
In \autoref{fig:loss_by_time}, this trade-off is illustrated by the validation loss over the training time.
For smaller batch sizes, the two effects cancel each other out, resulting in very similar improvements over time.
However, for larger batch sizes, the validation loss is visibly higher, as the reduced convergence carries more weight than the faster epoch times.

\begin{figure}[ht]
\centering
\includegraphics[width=2.5in]{../evaluation/diagrams-paper/validation-loss-by-time-and-bs--MNIST}
\caption{The validation loss by training time for different batch sizes using sequential training, LeNet5, and MNIST.
This figure demonstrates the trade-off where smaller batch sizes result in faster convergence per epoch but require more time per epoch.}
\label{fig:loss_by_time}
\end{figure}

For data parallelism, we generally try to increase the batch size as far as possible without losing too much quality.
Theoretically, the data parallel scalability can be arbitrarily high by using larger and larger batch sizes~\cite{krizhevsky2014-One-weird-trick} as a higher batch size improves the scalability.
% We revisit this impact of the batch size on the scalability when discussing the speedup in \autoref{sub:speedup}. % TODO do this!

% subsection impact_of_the_batch_size (end)

\subsection{Communication Time} % (fold)
\label{sub:communication_time}

We now compare the time spent on computation and communication.
In each epoch, we measure the time to feed-forward the training images, compute the gradients with back-propagation, and update the model with optimization steps as computation time.
The communication time is how long the processes spent on exchanging the gradients with all-reduce and computing the average gradient.

\begin{figure}[ht]
\centering
\includegraphics[width=2.5in]{../evaluation/diagrams-paper/epoch-time--LSDF-GPU-MNIST}
\caption{The ratio of computation to communication time per epoch for different batch sizes using parallel training on four GPUs, LeNet5, and MNIST.
As expected, both the computation and communication decreases the larger the batch size.
The reduction in computation time is in accordance with the results in \autoref{sub:impact_of_the_batch_size}.
Larger batches also result in less communication time, as there are fewer communication phases per epoch.}
\label{fig:comm_time_by_batch_size}
\end{figure}

\begin{figure*}[!t]
  \normalsize
  \centering
  \includegraphics[width=\textwidth]{../evaluation/diagrams-paper/epoch-time-bs-256--MNIST}
  \caption{The ratio of computation to communication time per epoch when training LeNet5 on MNIST using different machines and different numbers of processes.
  On LSDF CPU (Subfigure A), the behavior is as expected with the computation time decreasing and the communication time slightly increasing when using more processes.
  Interestingly, on the GPU (Subfigure B), not only the computation but also the communication time decreases when utilizing more GPUs.
  This initially counterintuitive observation might be explained by PCI-E piggybacking.
  The results on ForHLR II, see Subfigure C, show a decrease in computation time with more processes.
  However, for 8 or more processes, the communication time sharply increases.
  This might be an effect of ForHLR II's NUMA architecture.
  Nonetheless, this effect should not be as dramatic as currently measured.
  }
  \label{fig:comm_time_by_machine}
  \vspace*{4pt}
  \hrulefill
\end{figure*}

\autoref{fig:comm_time_by_batch_size} depicts the average computation and communication time per epoch using four GPUs.
Expectedly, the computation time decreases the larger the batch size.
This corresponds to the sequential results discussed in the previous \autoref{sub:impact_of_the_batch_size}.
The larger the batch size, the less computation per epoch is required.
%
Furthermore, larger batches also cause a decrease in communication time.
This is also a very expected result:
The larger each batch, the fewer batches there are in each epoch.
Since communication is required after each batch, larger batches, therefore, results in fewer communication phases in total, and thus less time spent communicating.


In \autoref{fig:comm_time_by_machine}, we compare the ratio between computation and communication over different machines and for different process counts.
In general, when using more processes, we would expect a decrease in computation time and an increase in communication time.

On LSDF using CPUs---that is, Subfigure A in \autoref{fig:comm_time_by_machine}---we observe precisely this expected effect.
The computation time is cut almost in half when using twice as many processes.
Compared to the computation, communication only makes up a small part of the training time.
This is explained by the fact that LSDF is a shared-memory machine which makes communication between different processes very efficient.
Still, when increasing the number of processes, there is a slight increase in communication time.

Subfigure B illustrates the training on LSFD's GPUs.
Again the computation time decreases the more GPUs we utilize.
However, counter-intuitively, there is also less time spent on communication.
%
To fully explain this observation, we would require more intense measurements---for example, measuring the communication separately for CPU-to-GPU, GPU-to-CPU, and CPU-to-CPU communication.

One possible cause for the decrease in communication time is PCI-E piggybacking.
To exchange the gradients across multiple GPUs with MPI, we first need to transfer them to the CPU, then communicate, and finally transfer the results back to the GPU.
Since the CPU-to-CPU communication is very fast on LSDF, see Subfigure A, most of the communication time is presumably spent on the transfers between the GPU and CPU.
%
The GPUs are connected via PCI-E, and always two GPUs share a PCI-E switch.
Packages sent via PCI-E must be acknowledged by the receiver.
PCI-E piggybacking occurs when these acknowledgments are attached to other packages, which improves the communication efficiency.
Additionally, the PCI-E switch might group multiple packages from its two GPUs together reducing the total package count.
As such, PCI-E might increase the communication efficiency and reduce the communication time when using more GPUs.

Finally, the results on ForHLR II are displayed in Subfigure C.
Unexpectedly, the communication increases drastically for more than four processes.
Again, a more detailed evaluation is required to be certain what causes this effect.
Partially, it might be caused by ForHLR II's NUMA architecture resulting in different communication speeds depending on the exact location of the core.
However, NUMA should not have such a dramatic effect as currently measured.
We, therefore, believe there to be some other, yet to determine, cause of this significant increase in communication time.

Another cause for the high communications times on both ForHLR II and LSDF's GPUs might be which of the sockets are used to train on.
Especially when using GPUs, the communication could be decelerated significantly when a GPU is assigned to a CPU on the ``wrong'' socket, which can increase the time to move gradients between this CPU-GPU pair.
This possible cause was proposed by the discussion after the talk to this paper.
However, we were not yet able to test it experimentally.

In addition to the communicational effects, we can observe the general speedup of GPU training over using CPUs.
When comparing Subfigures A and B in \autoref{fig:comm_time_by_machine}, despite the higher communication times, using GPUs is consistently more than five times faster than using CPUs on LSDF.

% subsection communication_time (end)

\subsection{Speedup} % (fold)
\label{sub:speedup}

\begin{figure*}[!t]
  \normalsize
  \centering
  \includegraphics[width=\textwidth]{../evaluation/diagrams-paper/speedup--LSDF-GPU-MNIST}
  \caption{The training speedup (solid with line points) and efficiency (dashed) for LeNet5 on MNIST for different batch sizes on up to four GPUs.
  Subplot A depicts the speedup (and efficiency) per epoch.
  As expected, larger batch sizes scale better due to the reduced communication.
  In Subplot B, we consider the total training time up to a certain threshold of the validation loss---in this case, $0.5$---is reached.
  We observe that the scalability is reduced compared to Subplot A.
  This means that the parallelization reduces the training improvement per epoch compared to the sequential training.}
  \label{fig:speedup}
  \vspace*{4pt}
  \hrulefill
\end{figure*}

We consider two different versions of the speedup.
First, we analyze the speedup per epoch, where we compare the training time per epoch for multiple processes to the training time for a single process.
The speedup per epoch for different batch sizes is displayed in Subfigure A of \autoref{fig:speedup}.
We achieve speedups of up to $3.21$ on four GPUs when using the highest batch size $512$.
As expected, larger batch sizes scale better due to the reduced communication per epoch we observed in \autoref{sub:communication_time}.

Subfigure B depicts the total training speedup.
As described in \autoref{sub:methodology}, we only consider the epochs until a certain loss threshold is reached.
When also considering the validation loss, the parallel training does not scale as good, reducing the maximum speedup on four GPUs to $2.60$.
This is due to the sequential training producing more improvement per epoch than the parallel version.
%
A more thorough investigation is required to fully determine why the parallelization impedes the training.
However, we believe that at least some of the lost accuracy can be attributed to our implementation of distributing and shuffling the training data.
As described in \autoref{sub:methodology}, each process shuffles its training data separately.
This reduces the overall shuffle variance, the more processes are used, yet this randomization is essential for the generalization of the model.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.3}
\caption{An Overview of Both the Total Speedup and the Speedup per Epoch with Batch Size 16 and 512 for the Different Hardware Variations.}
\label{tab:speedup}
\centering
\begin{tabular}{rR{8mm}R{8mm}R{8mm}R{8mm}}
\toprule
            & \multicolumn{2}{r}{\bfseries Epoch Speedup} & \multicolumn{2}{r}{\bfseries Total Speedup}\\
Batch Size            & 16                  & 512                       & 16                  & 512\\
\midrule
LSDF GPU, p=4         & 1.43                & 3.21                      & 0.95                & 2.60\\
LSDF CPU, p=16        & 6.83                & 7.70                      & 6.83                & 6.23\\
ForHLR II CPU, p=16   & 0.03                & 0.30                      & 0.03                & 0.37\\
\bottomrule
\end{tabular}
\end{table}

\autoref{tab:speedup} gives exact numbers for both the total speedup and the speedup per epoch on all three tested hardware variants.
In accordance with the high communication times observed in \autoref{sub:communication_time}, ForHLR II produces only devastating speedups of at most $0.37$ on $16$ processes.

% subsection speedup (end)

% section experimental_results (end)

\section{Conclusion} % (fold)
\label{sec:conclusion}

In this paper, we survey different techniques to parallelizing and distributing the training of neural networks, focusing primarily on convolutional neural networks for computer vision.
We discuss both model and data parallelism, including the different data parallel versions of synchronous and asynchronous SGD.
%
We implement one such synchronous SGD and evaluate it by training CNNs on the two image classification dataset MNIST and CIFAR10.
This implementation achieves decent speedups per epoch of up to $3.21$ on four GPUs and $7.70$ on 16 CPUs.
When comparing different batch sizes, we can confirm the established notion that large batch sizes reduce both the computation and communication per epoch, thus improving the scalability, but at the same time reduce the improvement per epoch.

We observe several interesting communication effects that raise further questions.
On the GPU, we measure a decrease in communication when using more GPUs, an effect we cannot fully explain at the moment, but which might be attributed to PCI-E piggybacking in the GPU-CPU-communication.
Furthermore, on ForHLR II, we observe a significant increase in communication for more than four processes.
%This might partially be explained by the NUMA architecture; however, NUMA should not cause delays that extreme.

Altogether, a more detailed evaluation than what is possible in this seminar would be necessary to investigate the observed effects thoroughly.
There are several aspects that could be considered in the future.
Besides a more comprehensive evaluation, the currently rather prototypical implementation could be improved to increase the performance and scalability.
Another task is implementing and comparing other parallelization approaches such as asynchronous data parallelism, model parallelism, and hybrids between those two.
%
The current experiments are limited to a small set of models and datasets.
Testing it on other types of CNNs, using larger training sets or even other types of computer vision task might give further insight into the scalability of data parallel training and establish the usability for a wider array of applications.
%
Finally, one could investigate parallelizing other areas of neural network applications.
There have, for example, been multiple works showing that networks for speech processing are significantly harder to parallelize and scale than computer vision models~\cite{seide2014-Speech-DNNs,seide2014-1-bit-stochastic,dean2012-Large-scale-distributed}.


% summarize contributions, reference to introduction/abstract
% summarize central results and implications
% revisit section key points
% Benefits and shortcomings of approach and results

% section conclusion (end)

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

% \vfill\null
% \newpage

\appendix[Additional Experimental Results]
We also evaluated our implementation using the TorchNet model and CIFAR10 dataset.
Since those results regarding parallelism are fairly similar to training LeNet5 on MNSIT, they are displayed in this appendix.

\autoref{fig:validation_loss_model_dataset} gives the validation loss per epoch for LeNet5 and TorchNet on both MNIST and CIFAR10.
We observe that MNIST requires fewer epochs than CIFAR10 to converge and reaches a better overall accuracy even after training for five times as many epochs.
No significant difference between the two models, LeNet5 and TorchNet, is visible.
Since they a fairly similar architecture, this is somewhat expected.

% \vspace{-2mm}
\begin{figure}[ht]
\centering
\includegraphics[width=2.5in]{../evaluation/diagrams-paper/validation-loss-by-model-and-dataset--bs-256}
\caption{The sequential validation loss per epoch.
Training on the MNIST dataset requires significantly fewer epochs than CIFAR10 while also achieving higher overall accuracy.
For the models, LeNet5 generally performs slightly better in the first few epochs, while TorchNet achieves higher final accuracy.}
\label{fig:validation_loss_model_dataset}
\end{figure}
% \vspace{-1mm}

The speedup per epoch is given in \autoref{fig:speedup_model_dataset}.
All four tested model-dataset combination scale similarly well.
The observations made in \autoref{sec:experimental_results} for LeNet5 and MNIST are therefore also applicable to TorchNet and CIFAR10.
% \vspace{-2mm}

\begin{figure}[!ht]
\centering
\includegraphics{../evaluation/diagrams-paper/speedup-all--bs-256}
\caption{The speedup and efficiency per epoch on the GPU using all combinations of the two models and datasets with batch size 256.
All four variants demonstrate very similar parallel behavior.}
\label{fig:speedup_model_dataset}
\end{figure}


\end{document}
